---
title: "Bike Rental Analysis"
author: "Brian Boughton"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
---

# Milestone 1 - Data Introduction

```{r}
start<-Sys.time()
```

```{r}
pacman::p_load(pacman, dplyr, forecast,car,ROSE,caret,FNN,fastDummies,neuralnet,e1071)
options(scipen=25)
set.seed(13)
```

## Introduction
The data set used was collected by Hadi fanaee from the laboratory of Artificial Intelligence and Decision Support at the University of Porto in Portugal. The contains information around bike sharing. It consist of an entry number, date, year, season, month, day of the week, hour, and whether or not it was a weekend. It also contains information around weather like temperature, wind speed and, forecast. Lastly, it gives the amount of casual and registered bike borrows along the total number.

The final data frame is called simply pj for project and there are other version of the dataset if needed.
Description of each feature:

* instant: record index

* dteday : date

* season : season (1:spring, 2:summer, 3:fall, 4:winter)

* yr : year (0: 2011, 1:2012)

* mnth : month ( 1 to 12)

* hr : hour (0 to 23)

* holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)

* weekday : day of the week

* workingday : if day is neither weekend nor holiday is 1, otherwise is 0.

* weathersit : 

* - 1: Clear, Few clouds, Partly cloudy, Partly cloudy

*	- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist

*	- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds

*	- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog

* temp : Normalized temperature in Celsius. The values are divided to 41 (max)

* atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)

* hum: Normalized humidity. The values are divided to 100 (max)

* windspeed: Normalized wind speed. The values are divided to 67 (max)

* casual: count of casual users

* registered: count of registered users

* cnt: count of total rental bikes including both casual and registered



## Goal

Our goal with this data is to make predictions of future rentals based on day information and predicted forecasts. There are a few unnecessary
variables in the dataset that have been removed like the instant number and date along with the casual and registered users because we are currently only interested in the overall users.


## Data Wrangling

### Importing data, setting factors and removing the uneeded variables.
```{r}
pj.df<-read.csv("hour.csv")
pj.df$season<- as.factor(pj.df$season)
pj.df$yr<- as.factor(pj.df$yr)
pj.df$mnth<- as.factor(pj.df$mnth)
pj.df$hr<- as.factor(pj.df$hr)
pj.df$holiday<- as.factor(pj.df$holiday)
pj.df$weekday<- as.factor(pj.df$weekday)
pj.df$workingday<- as.factor(pj.df$workingday)
pj.df$weathersit<- as.factor(pj.df$weathersit)
pj<-subset(pj.df, select = -c(instant, dteday,casual,registered))
```

### Getting to understand the data.

```{r}
summary(pj)
#showing number of Entries and features
dim(pj)
#displaying the number of missing entries
sum(is.na(pj))
#showing numeric data features
names(select_if(pj, is.numeric))
#showing categorical data features
names(select_if(pj, is.factor))
```

We can see that the data consist of 13 different variables and 17,379 observations. There is not missing data entries found within the data. The numeric features are temp, atemp, hum, windspeed, and cnt. the categorical features are season, yr, mnth, hr, holiday, weekday, workingday, and weathersit.

### Creating a categorical dataset

The dependent variable in this dataset is a numerical value but I will be using logistic models models to predict the dataset so I will convert the cnt feature to 0 or 1 based on whether or not the value is above or below the mean conditioned by the season.

```{r}
#Making boxplot showing relationship between cnt and season
with(pj, plot(cnt~season, main="Box Plot of cnt vs. season", ylab = "# of bikes rented", xlab="Seasons"))
```

In the box plot above I can see that the means between season are quite different so Ican condition the mean around those mean values.

```{r}
#Finding means conditioned by the season
mean.spring<-mean(pj[pj$season == 1, 'cnt'], na.rm = TRUE)
mean.summer<-mean(pj[pj$season == 2, 'cnt'], na.rm = TRUE)
mean.fall<-mean(pj[pj$season == 3, 'cnt'], na.rm = TRUE)
mean.winter<-mean(pj[pj$season == 4, 'cnt'], na.rm = TRUE)

#replacing 'cnt' entries with either '0' or '1' based on their value relative to the conditioned mean.
pj$cnt<-ifelse(pj$cnt >= mean.spring & pj$season == '1', 1,
        ifelse(pj$cnt >= mean.summer & pj$season == '2', 1,
        ifelse(pj$cnt >= mean.fall   & pj$season == '3', 1,
        ifelse(pj$cnt >= mean.winter & pj$season == '4', 1,
        ifelse(pj$cnt  < mean.spring & pj$season == '1', 0,     
        ifelse(pj$cnt  < mean.summer & pj$season == '2', 0,
        ifelse(pj$cnt  < mean.fall   & pj$season == '3', 0
               ,0)))))))
#Looking at what percentage of the entries for 'cnt' is 1 compared the whole data set.
sum(pj$cnt)
length(pj$cnt)
sum(pj$cnt)/length(pj$cnt)
```

Now  I have successfully converted from numerical values for cnt to categorical values where about 40% of the entries are 1s and the remaining a 0s.


## Conclusion

Goal: is to be able to predict whether or not the hourly rental of bikes will be above or below the mean conditioned by the season of the year.

Data Alterations: The data was imputed to alter the numeric numbers in the "cnt" feature to be a categorical value of 0 for below or equal to mean rentals based on season and 1 for above.

Next Step: is to build a logistic model based on the above data.


# Milestone 2 - Logistic Regression

## Goal:
  
  1. a classification model with training-test split of 60- 40 and results of precision, recall, and f-measure
  2. a classification model with training-test split of 80- 20 and results of precision, recall, and f-measure
  3. Bonus, if you report the result of ROC curve and calculate the AUC score. (5%)

## Model development

### Data Seperation

Because the data is ordered,I must use a randomized method to separate the data into training and validation sets.I am using the seed **13** to allow for repeatable results.

```{r}
#Partitioning data into 60% training and 40% validation data 
train.index1 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.6)  
train.df1 <- pj[train.index1, ]
valid.df1 <- pj[-train.index1, ]
#Partitioning data into 80% training and 20% validation data 
train.index2 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.8)  
train.df2 <- pj[train.index2, ]
valid.df2 <- pj[-train.index2, ]
```


```{r}
#verifying that training data is equal to 60% and that training plus valid is same length as whole data set 
length(train.df1$cnt)/length(pj$cnt)
length(train.df1$cnt)+length(valid.df1$cnt) == length(pj$cnt)
#verifying that training data is equal to 80% and that training plus valid is same length as whole data set 
length(train.df2$cnt)/length(pj$cnt)
length(train.df2$cnt)+length(valid.df2$cnt) == length(pj$cnt)
```
Now that I have successfully partitioned the data, I can build optimal logit models through the stepwise method.
```{r}
#Generating null and full models for both data partitions
lognull1<-glm(data=valid.df1, cnt~1, family = "binomial")
logfull1<-glm(data=valid.df1, cnt~., family = "binomial")
lognull2<-glm(data=valid.df2, cnt~1, family = "binomial")
logfull2<-glm(data=valid.df2, cnt~., family = "binomial")
```

```{r}
#Find optimal model based on 60/40 data partition.
opt.logit1<-step(
    logfull1,
    scope = list(upper = logfull1, lower = lognull1),
    direction = "both",
    trace = FALSE, family = binomial
  )
summary(opt.logit1)
#Finding optimal model based on 80/20 data partition.
opt.logit2<-step(
    logfull2,
    scope = list(upper = logfull2, lower = lognull2),
    direction = "both",
    trace = FALSE, family = binomial
  )
summary(opt.logit2)
```
## Evaluation of the models.

### 60/40 partition logit model analysis.

To begin the analysis I will create a confusion matrix that will compare the models predicted values to the actual values based on the training and validation data sets. 
```{r}
actual.train1<-train.df1$cnt
actual.valid1<-valid.df1$cnt
pred.train1<-predict(opt.logit1,train.df1[,-13],type='response')
pred.valid1<-predict(opt.logit1,newdata=valid.df1[,-13],type='response')
(conf.matrix1<-table(actual.train1, pred.train1>.5))
(conf.matrix2<-table(actual.valid1, pred.valid1>.5))
```

### Accuracy
```{r}
(acc.train1<-sum(conf.matrix1[2,2],conf.matrix1[1,1])/sum(conf.matrix1))
(acc.valid1<-sum(conf.matrix2[2,2],conf.matrix2[1,1])/sum(conf.matrix2))
```

### Precision
```{r}
(prec.train1<-conf.matrix1[2,2]/(conf.matrix1[2,2]+conf.matrix1[1,2]))
(prec.valid1<-conf.matrix2[2,2]/(conf.matrix2[2,2]+conf.matrix2[1,2]))
```

### Recall
```{r}
(rec.train1<-conf.matrix1[2,2]/(conf.matrix1[2,2]+conf.matrix1[2,1]))
(rec.valid1<-conf.matrix2[2,2]/(conf.matrix2[2,2]+conf.matrix2[2,1]))
```
### F-stat
```{r}
(stat.train1<-(2*prec.train1*rec.train1)/(prec.train1 + rec.train1))
(stat.valid1<-(2*prec.valid1*rec.valid1)/(prec.valid1 + rec.valid1))
```


### AUC and ROC Curves
```{r}
(roc.train1<-roc.curve(actual.train1, pred.train1))
(roc.valid1<-roc.curve(actual.valid1, pred.valid1))
```

Based on the analysis the model seems to preform pretty well due to the similarities between the training a validation data sets.


### 80/20 partition logit model analysis.

To begin the analysis I will create a confusion matrix that will compare the models predicted values to the actual values based on the training and validation data sets. 
```{r}
actual.train2<-train.df2$cnt
actual.valid2<-valid.df2$cnt
pred.train2<-predict(opt.logit2,train.df2[,-13],type='response')
pred.valid2<-predict(opt.logit2,newdata=valid.df2[,-13],type='response')
(conf.matrix3<-table(actual.train2, pred.train2>.5))
(conf.matrix4<-table(actual.valid2, pred.valid2>.5))
```

### Accuracy
```{r}
(acc.train2<-sum(conf.matrix3[2,2],conf.matrix3[1,1])/sum(conf.matrix3))
(acc.valid2<-sum(conf.matrix4[2,2],conf.matrix4[1,1])/sum(conf.matrix4))
```

### Precision
```{r}
(prec.train2<-conf.matrix3[2,2]/(conf.matrix3[2,2]+conf.matrix3[1,2]))
(prec.valid2<-conf.matrix4[2,2]/(conf.matrix4[2,2]+conf.matrix4[1,2]))
```

### Recall
```{r}
(rec.train2<-conf.matrix3[2,2]/(conf.matrix3[2,2]+conf.matrix3[2,1]))
(rec.valid2<-conf.matrix4[2,2]/(conf.matrix4[2,2]+conf.matrix4[2,1]))
```
### F-stat
```{r}
stat.train2<-(2*prec.train2*rec.train2)/(prec.train2 + rec.train2)
stat.valid2<-(2*prec.valid2*rec.valid2)/(prec.valid2 + rec.valid2)
```


### AUC and ROC Curves
```{r}
(roc.train2<-roc.curve(actual.train2, pred.train2))
(roc.valid2<-roc.curve(actual.valid2, pred.valid2))
```

Based on the analysis the model seems to preform pretty well due to the similarities between the training a validation data sets.

### Model comparison

First I will create a table to easily see all the analysis in one spot.

```{r}
table1<-cbind(
  c(acc.train1,prec.train1,rec.train1,stat.train1,roc.train1$auc), 
  c(acc.train2,prec.train2,rec.train2,stat.train2,roc.train2$auc),
  c(acc.valid1,prec.valid1,rec.valid1,stat.valid1,roc.valid1$auc),
  c(acc.valid2,prec.valid2,rec.valid2,stat.valid2,roc.valid2$auc)
  )
colnames(table1)<-c("Model1.Train","Model2.Train","Model1.Valid","Model2.Valid")
rownames(table1)<-c("Accuracy","Precision","Recall","F-Stat","AUC")
table1<-data.frame(table1)
table1
```
*Accuracy:* Model 2 did better with both training and validation data
*Precision:* Model 2 did better with both training and validation data
*Recall:* Model 2 did better with both training and validation data
*F-Stat* Model 2 did better with both training and validation data
*AIC*  Model1 train was better but model2 validation was better

### Conclusion

I created a 60/40 split of data and build a glm model. I used the Stepwise in both directions to attempt to find the model with the best fit.
The model build was: 
cnt ~ season + yr + mnth + hr + holiday + weekday + weathersit + temp + atemp + hum + windspeed, family + "binomial", data =valid.df1

I then made a confusion matrix to do analysis on the model performance and test involving accuracy, precision, recall, F-stat and AIC.
Looking at the results I decided the model was good at predicting how the next days rentals will be. 

Next, I created a 80/20 split of data and build a glm model. I used the Stepwise in both directions to attempt to find the model with the best fit.The model build was:
cnt ~ season + yr + hr + holiday + weekday + weathersit + atemp + hum, family = "binomial", data = valid.df2

I then made a confusion matrix to do analysis on the model performance and test involving accuracy, precision, recall, F-stat and AIC.
Looking at the results I decided the model was good at predicting how the next days rentals will be

It seems that Model2 is the stronger performer. This may be due to missing features because though some of the levels of the features like "mnth" only have a high significance level for 3 of the variables. Model 1 therefore could have suffered from overfitting and ultimately lead to model 2 being the stronger performer.

# Milestone 3 - KNN

## Goal

1. a classification model with training-test split of 80- 20 and results of precision, recall, and f-measure

2. a classification model with training-test split of 60- 40 and results of precision, recall, and f-measure

3. Bonus, if you report the result of ROC curve and calculate the AUC score. (5%)

## Data Wrangeling

### Reimporting data and defining binomial value for "cnt"

Because the dataset numerical for knn and also there will fewer attributes I decided it was simpler to just import the data again.
```{r}
#reimporting data
pj <- read.csv("hour.csv")
mean.spring<-mean(pj[pj$season == 1, 'cnt'], na.rm = TRUE)
mean.summer<-mean(pj[pj$season == 2, 'cnt'], na.rm = TRUE)
mean.fall<-mean(pj[pj$season == 3, 'cnt'], na.rm = TRUE)
mean.winter<-mean(pj[pj$season == 4, 'cnt'], na.rm = TRUE)
pj$cnt<-ifelse(pj$cnt >= mean.spring & pj$season == '1', 1,
        ifelse(pj$cnt >= mean.summer & pj$season == '2', 1,
        ifelse(pj$cnt >= mean.fall   & pj$season == '3', 1,
        ifelse(pj$cnt >= mean.winter & pj$season == '4', 1,
        ifelse(pj$cnt  < mean.spring & pj$season == '1', 0,     
        ifelse(pj$cnt  < mean.summer & pj$season == '2', 0,
        ifelse(pj$cnt  < mean.fall   & pj$season == '3', 0
               ,0)))))))

```

### Attribute and dummifaction

I decided to go with weathersit, atemp, and hum to build the knn model. This is because these attributes where strong performers in the logistic regression model and will require fewer dummy variables.

```{r}
#Only using weathersit, atemp, and hum to predict cnt
pj<-subset(pj, select = c(weathersit, atemp,hum,cnt))
#remove all lines containing weathersit == 4 because of low sample size. 
pj<- pj[pj$weathersit !=4,]
#creating dummy variables for weathersit.
pj <- dummy_cols(pj,select_columns = "weathersit")
pj <- subset(pj, select = c(weathersit_1,weathersit_2,weathersit_3,atemp,hum,cnt ))
summary(pj$weathersit_1)
pj<-pj%>%
  rename("sunny" = "weathersit_1",
         "cloudy" = "weathersit_2",
         "rainy" = "weathersit_3")
```

### Partitioning

Like the previous model, there will be two different partitions. One will be a 60-40 split and the other 80-20 split.

```{r}
#Partitioning data into 60% training and 40% validation data 
train.index1 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.6)  
train.df1 <- pj[train.index1, ]
valid.df1 <- pj[-train.index1, ]
#Partitioning data into 80% training and 20% validation data 
train.index2 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.8)  
train.df2 <- pj[train.index2, ]
valid.df2 <- pj[-train.index2, ]
#defining actual values for model evaluation
actual.valid1<-valid.df1$cnt
actual.valid2<-valid.df2$cnt
```

## Model Development

### Defining range for k and table development

The recommended range to search for the optimal k is the square root of the length of the data set. The accuracy.df will log the performance of k at each value between 1 and n=sqrt(length(df)) 
```{r}
(n=round(sqrt(length(train.df1$cnt)),0))
(n1=round(sqrt(length(train.df2$cnt)),0))
accuracy.df1<-data.frame(k = seq(1,n,1), accuracy = rep(0,n)) 
accuracy.df2<-data.frame(k = seq(1,n1,1), accuracy = rep(0,n1))
```

### Finding optimal k

Each model will now checked through a loop find their performance at each value of k. The training model is the 60% data set and the testing model is the 40% validation set. 

```{r}
for(i in 1:n) {
  knn.pred1 <- knn(train.df1[, 1:5], valid.df1[, 1:5], cl = train.df1[, 6], k = i)
  accuracy.df1[i, 2] <- confusionMatrix(as.factor(knn.pred1), as.factor(valid.df1[, 6]))$overall[1] 
}
```

Each model will now checked through a loop find their performance at each value of k. The training model is the 80% data set and the testing model is the 20% validation set. 

```{r}
for(i in 1:n1) {
  knn.pred2 <- knn(train.df2[, 1:5], valid.df2[, 1:5], cl = train.df2[, 6], k = i)
  accuracy.df2[i, 2] <- confusionMatrix(as.factor(knn.pred2), as.factor(valid.df2[, 6]))$overall[1] 
}
```

## Evaluation

### 60-40 data perfomance

We will define k1 as the k value that had the highest performance accuracy during the model development. The knn.best1 will use the value given to k1 to develop the best model and then a confusion matrix will be build along with a roc.curve to display performance.

```{r}
#The optimal k value
(k1<-accuracy.df1[which.max(accuracy.df1$accuracy),])
knn.best1 <- knn(train.df1[, 1:5], valid.df1[, 1:5], cl = train.df1[, 6], k = k1$k)
(matrix1<-confusionMatrix(as.factor(knn.best1), as.factor(valid.df1[,6])))
(roc.knn1 <- roc.curve(actual.valid1, knn.best1))
```

### 80-20 data perfomance

We will define k2 as the k value that had the highest performance accuracy during the model development. The knn.best2 will use the value given to k2 to develop the best model and then a confusion matrix will be build along with a roc.curve to display performance.

```{r}
#The optimal k value
(k2<-accuracy.df2[which.max(accuracy.df2$accuracy),])
knn.best2 <- knn(train.df2[, 1:5], valid.df2[, 1:5], cl = train.df2[, 6], k = k2$k)
(matrix2<-confusionMatrix(as.factor(knn.best2), as.factor(valid.df2[,6])))
(roc.knn2 <- roc.curve(actual.valid2, knn.best2))
```


## Conclusion

### Table development

I am going to build another table to summerize the findings of the knn models.

```{r}
table2<-cbind(
  c(matrix1$overall[1],matrix1$byClass[5],matrix1$byClass[1],
      (2*matrix1$byClass[5]*matrix1$byClass[1])/(matrix1$byClass[5] + matrix1$byClass[1]),roc.knn1$auc),
  c(matrix2$overall[1],matrix2$byClass[5],matrix2$byClass[1],
      (2*matrix2$byClass[5]*matrix2$byClass[1])/(matrix2$byClass[5] + matrix2$byClass[1]),roc.knn2$auc))
colnames(table2)<-c("knn1","knn2")
rownames(table2)<-c("Accuracy","Precision","Recall","F-Stat","AUC")
(table2<-data.frame(table2))
```
### Summary

Looking at the summary table we see that the 60-40 knn1 model preformed better than the 80-20 knn2 model in the five categories above. This is probably due to the larger valid data set giving it more information to find an optimal k. Overall it seems that logistic model performs stronger than the knn model. This could be due to poor feature selection.


# Milestone 4 - Neural Network

## Goal

1. a classification model with training-test split of 80- 20 and results of precision, recall, and f-measure

2. a classification model with training-test split of 60- 40 and results of precision, recall, and f-measure

3. Bonus, if you report the result of ROC curve and calculate the AUC score. (5%)


## Data Wrangeling

### importing data and data cleaning

Neural networks make a lot more assumptions and we will require two output features so it is easier to start with the raw data set again.


```{r}
pj<-read.csv("day.csv")
pj <- pj[pj$weathersit != 4,]
mean.spring<-mean(pj[pj$season == 1, 'cnt'], na.rm = TRUE)
mean.summer<-mean(pj[pj$season == 2, 'cnt'], na.rm = TRUE)
mean.fall<-mean(pj[pj$season == 3, 'cnt'], na.rm = TRUE)
mean.winter<-mean(pj[pj$season == 4, 'cnt'], na.rm = TRUE)
pj$cnt<-ifelse(pj$cnt >= mean.spring & pj$season == '1', 1,
        ifelse(pj$cnt >= mean.summer & pj$season == '2', 1,
        ifelse(pj$cnt >= mean.fall   & pj$season == '3', 1,
        ifelse(pj$cnt >= mean.winter & pj$season == '4', 1,
        ifelse(pj$cnt  < mean.spring & pj$season == '1', 0,     
        ifelse(pj$cnt  < mean.summer & pj$season == '2', 0,
        ifelse(pj$cnt  < mean.fall   & pj$season == '3', 0,
               0)))))))
pj$above<-ifelse(pj$cnt == 1,1,0)
pj$below<-ifelse(pj$cnt == 0,1,0)
pj<-dummy_columns(pj, select_columns = c("weathersit", "season", "mnth", "weekday"))
pj<-pj%>%
  rename("sunny"     =   "weathersit_1",
         "cloudy"    =   "weathersit_2",
         "rainy"     =   "weathersit_3",
         "spring"    =   "season_1",
         "summer"    =   "season_2",
         "fall"      =   "season_3",
         "winter"    =   "season_4",
         "jan"       =   "mnth_1",
         "feb"       =   "mnth_2",
         "mar"       =   "mnth_3",
         "apr"       =   "mnth_4",
         "may"       =   "mnth_5",
         "jun"       =   "mnth_6",
         "jul"       =   "mnth_7",
         "aug"       =   "mnth_8",
         "sep"       =   "mnth_9",
         "oct"       =   "mnth_10",
         "nov"       =   "mnth_11",
         "dec"       =   "mnth_12",
         "sun"       =   "weekday_0",
         "mon"      =   "weekday_1",
         "tues"      =   "weekday_2",
         "wends"     =   "weekday_3",
         "thurs"       =   "weekday_4",
         "fri"       =   "weekday_5",
         "sat"       =   "weekday_6"
         )
```

### Feature Selection

```{r}
colnames(pj)
pj<-subset(pj, select = -c(instant, dteday,casual,registered,season,yr,mnth,holiday,weathersit,casual,registered))
```

### Data splitting

```{r}
#Partitioning data into 60% training and 40% validation data 
train.index1 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.6)  
train.df1 <- pj[train.index1, ]
valid.df1 <- pj[-train.index1, ]
#Partitioning data into 80% training and 20% validation data 
train.index2 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.8)  
train.df2 <- pj[train.index2, ]
valid.df2 <- pj[-train.index2, ]
actual.valid1<-valid.df1$cnt
actual.valid2<-valid.df2$cnt
```

## Model Development

Now, we will use the neuralnet function to develop an neural network using the 60-40 and 80-20 data sets.

```{r}
#60-40 dataset
set.seed(13)
nn1 <- neuralnet(above + below ~  rainy + cloudy + sunny + spring + summer + winter + fall + temp + atemp + hum + workingday, data = train.df1, hidden = 1, linear.output = T, stepmax = 10000000)
plot(nn1)
```

```{r}
#80-20 data set
set.seed(13)
nn2 <- neuralnet(above + below ~   rainy + cloudy + sunny + spring + summer + winter + fall + temp + atemp + hum + workingday, data = train.df2, hidden = 5, linear.output = T, stepmax = 10000000)
plot(nn2)
```

## Evaluation

Using the neuralnet packages "compute" function, we can used the developed model to check how it performs on our validation data sets.

```{r}
#60-40
predict1 <- neuralnet::compute(nn1, data.frame(
  valid.df1$rainy,
  valid.df1$cloudy,
  valid.df1$sunny,
  valid.df1$spring,
  valid.df1$summer,
  valid.df1$winter,
  valid.df1$fall,
  valid.df1$temp,
  valid.df1$atemp,
  valid.df1$hum,
  valid.df1$workingday
  ))
predict.cl1=apply(predict1$net.result,1,which.max)-1
(mat1<-confusionMatrix(as.factor(ifelse(predict.cl1==1,0,1)),as.factor(valid.df1$cnt)))
```

```{r}
#80-20 split
predict2 <- neuralnet::compute(nn2, data.frame(
  valid.df2$rainy,
  valid.df2$cloudy,
  valid.df2$sunny,
  valid.df2$spring,
  valid.df2$summer,
  valid.df2$winter,
  valid.df2$fall,
  valid.df2$temp,
  valid.df2$atemp,
  valid.df2$hum,
  valid.df2$workingday
  ))
predict.cl2=apply(predict2$net.result,1,which.max)-1
(mat2<-confusionMatrix(as.factor(ifelse(predict.cl2==1,0,1)),as.factor(valid.df2$cnt)))
```

Now to compute the ROC curve and AUC information. 
```{r}
(roc.nn1<-roc.curve(actual.valid1, predict.cl1))
```

```{r}
(roc.nn2<-roc.curve(actual.valid2, predict.cl2))
```

## Conclusion

### Table development

```{r}
table3<-cbind(
  c(mat1$overall[1],mat1$byClass[5],mat1$byClass[1],
      (2*mat1$byClass[5]*mat1$byClass[1])/(mat1$byClass[5] + mat1$byClass[1]),roc.nn1$auc),
  c(mat2$overall[1],mat2$byClass[5],mat2$byClass[1],
      (2*mat2$byClass[5]*mat2$byClass[1])/(mat2$byClass[5] + mat2$byClass[1]),roc.nn2$auc))
colnames(table3)<-c("nn1","nn2")
rownames(table3)<-c("Accuracy","Precision","Recall","F-Stat","AUC")
(table3<-data.frame(table3))
```

It appears that the neural network that used the 80-20 data split out performed the 60-40 data set. Overall, the models under performed the other two models. This may be because their is a strong linear relationship between the data.  


# Milestone 5

# Milestone 5 -Naive Bayes

## Goal

Implement NB model with 80-20 and 60-40 split evaluation. If you implement 4-fold cross-validation analysis, you will get 10-point bonus.


* another classification model with training-test split of 80-20 and 60- 40 and results of precision, recall, and f-measure. (Note, if you work as a group, you need to pick another algorithm different from your Milestone 2 with another feature set after the feature selection.)

* A model on 4-fold cross-validation with the results of precision, recall, and f-measure. (Bonus)

* a report to summarize your finding. It may include but not limit to the questions, such as which model performances better in this dataset and which split performs better? Why?


The goal will be to use Naive Bayes to predict whether or not bike rentals will go above the mean number of hourly rentals based on any given conditions.


## Importing data and defining binomial value for "cnt"


```{r}
#Importing data
pj <- read.csv("hour.csv")

#Defining the conditional mean to determine values for 'cnt'
mean.spring<-mean(pj[pj$season == 1, 'cnt'], na.rm = TRUE)
mean.summer<-mean(pj[pj$season == 2, 'cnt'], na.rm = TRUE)
mean.fall<-mean(pj[pj$season == 3, 'cnt'], na.rm = TRUE)
mean.winter<-mean(pj[pj$season == 4, 'cnt'], na.rm = TRUE)
#changing numerical values of 'cnt' to categorical values 0 = 'below conditional mean 1 = 'above'
pj$cnt<-ifelse(pj$cnt >= mean.spring & pj$season == '1', 1,
        ifelse(pj$cnt >= mean.summer & pj$season == '2', 1,
        ifelse(pj$cnt >= mean.fall   & pj$season == '3', 1,
        ifelse(pj$cnt >= mean.winter & pj$season == '4', 1,
        ifelse(pj$cnt  < mean.spring & pj$season == '1', 0,     
        ifelse(pj$cnt  < mean.summer & pj$season == '2', 0,
        ifelse(pj$cnt  < mean.fall   & pj$season == '3', 0
               ,0)))))))
# converting other numerical values to categorical using unconditional mean
pj$temp<-ifelse(pj$temp >= mean(pj$temp), 1, 0)
pj$atemp<-ifelse(pj$atemp >= mean(pj$atemp), 1, 0)
pj$hum<-ifelse(pj$hum >= mean(pj$hum), 1, 0)
pj$windspeed<-ifelse(pj$windspeed >= mean(pj$windspeed), 1, 0)
pj$weathersit<-ifelse(pj$weathersit == 4,3,pj$weathersit)
pj$cnt<-ifelse(pj$cnt >= mean(pj$cnt), 1, 0)
#Removing unwanted features  
pj<-subset(pj, select = -c(instant, dteday,casual,registered,yr))
# turning all features to categorical 
pj[1:12]<-lapply(pj[1:12],factor)
head(pj)
```
### Partitioning

Like the previous model, there will be two different partitions. One will be a 60-40 split and the other 80-20 split.

```{r}
#Partitioning data into 60% training and 40% validation data 
train.index1 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.6)  
train.df1 <- pj[train.index1, ]
valid.df1 <- pj[-train.index1, ]
#Partitioning data into 80% training and 20% validation data 
train.index2 <- sample(c(1:dim(pj)[1]), dim(pj)[1]*0.8)  
train.df2 <- pj[train.index2, ]
valid.df2 <- pj[-train.index2, ]
#defining actual values for model evaluation
actual.valid1<-valid.df1$cnt
actual.valid2<-valid.df2$cnt
```

## Building the model

Since Naive Bayes is easily computed relative to Neural Network and KNN, I will be using all the relevant features from the dataset. 

```{r}
#60-40 training data set
nb1<-naiveBayes(cnt~., data=train.df1)
#80-20 training data set
nb2<-naiveBayes(cnt~., data=train.df2)
```

## prediction

Now that I have built a model based on the training dataset, we can now use it to make predictions on the validation data set. Then it will classify the probability where if the probability is less than .5 it will be zero otherwise it is 1.

```{r}
pred.prob1 <- predict(nb1, newdata = valid.df1, type = "raw")
pred.class1 <- ifelse(pred.prob1[,1]>0.5, 1, 0)

pred.prob2 <- predict(nb2, newdata = valid.df2, type = "raw")
pred.class2 <- ifelse(pred.prob2[,1]>0.5, 1, 0)
```

## Evaluation

## 60-40 data set evaluation

To evaluate the models performances, we will use confusion matrices and measures Accuracy, Precision, Recall, and F-stat.
After will look at the ROC curve and the the AUC Value.

**Confusion Matrix**
```{r}
# validation
pred.prob1 <- predict(nb1, newdata = valid.df1, type="raw")
pred.class1 <- ifelse(pred.prob1[,1]>0.5, 0, 1)
nb.cm1<-confusionMatrix(as.factor(pred.class1), as.factor(valid.df1$cnt))
```

**ROC Curve**

```{r}
roc.nb1<-roc.curve(actual.valid1,pred.class1)
```


### 80-20 data set evaluation

```{r}
# validation
pred.prob2 <- predict(nb2, newdata = valid.df2, type="raw")
pred.class2 <- ifelse(pred.prob2[,1]>0.5, 0, 1)
nb.cm2<-confusionMatrix(as.factor(pred.class2), as.factor(valid.df2$cnt))
```

**ROC Curve**

```{r}
roc.nb2<-roc.curve(actual.valid2,pred.class2)
```

### Table

To easily compare the data I will build a table with the performance values and I will pick the best one.

```{r}
table4<-cbind(
  c(nb.cm1$overall[1],nb.cm1$byClass[5],nb.cm1$byClass[1],
      (2*nb.cm1$byClass[5]*nb.cm1$byClass[1])/(nb.cm1$byClass[5] + nb.cm1$byClass[1]),roc.nb1$auc),
  c(nb.cm2$overall[1],nb.cm2$byClass[5],nb.cm2$byClass[1],
      (2*nb.cm2$byClass[5]*nb.cm2$byClass[1])/(nb.cm2$byClass[5] + nb.cm2$byClass[1]),roc.nb2$auc))
colnames(table4)<-c("NB_Model_1","NB_Model_2")
rownames(table4)<-c("Accuracy","Precision","Recall","F-Stat","AUC")
table4<-data.frame(table4)
```

### Conclusion

It appears that the first model performed stronger than the second model. This may be to the 80-20 data set being over tuned due to the size of the data set.

Now, we will compare all the models to see what one performs the best.

```{r}
table5<-cbind(
c(acc.valid2,prec.valid2,rec.valid2,stat.valid2,roc.valid2$auc),
c(matrix1$overall[1],matrix1$byClass[5],matrix1$byClass[1],
    (2*matrix1$byClass[5]*matrix1$byClass[1])/(matrix1$byClass[5] + matrix1$byClass[1]),roc.knn1$auc),
c(mat2$overall[1],mat2$byClass[5],mat2$byClass[1],
    (2*mat2$byClass[5]*mat2$byClass[1])/(mat2$byClass[5] + mat2$byClass[1]),roc.nn2$auc),
c(nb.cm1$overall[1],nb.cm1$byClass[5],nb.cm1$byClass[1],
    (2*nb.cm1$byClass[5]*nb.cm1$byClass[1])/(nb.cm1$byClass[5] + nb.cm1$byClass[1]),roc.nb1$auc))
colnames(table5)<-c("Logit","KNN","NN","NB")
rownames(table5)<-c("Accuracy","Precision","Recall","F-Stat","AUC")
table5<-data.frame(table5)
table5
```

Logistic performed best in Accuracy, recall, and AUC

Naive Bayes performed best in Precision and F-stat

It makes sense that these two models would be the best performers due to them using the most amount of features in the model. Logistic regressions performance can be explained by its ability to use categorical and numeric values in its calculations. This allowed for it to use the data in a more organic state. 

The conversion of numeric values into categorical values may have helped Naive Bayes performance in precision by making the data more simple.


# Milestone 6 - Hierarchical Clustering

## Goal

### Requirements

* Implement Hierarchical Clustering model

* Display and summarize findings.

This model is going to be done a bit differently than previous models. I will be making clusters based on the month instead of the count of bikes. I am also going to be using casual and registered instead of the cnt total.

### Scenario

The bike rental company wants to find which months to do a discount promotion

## Data Wrangling

The data set is going to have to be turned into a data set with 12 entries and 11 features. The 12 entries will be one for each month and the features will be either the percentage or average of that features for each month.

### Reimportation

The data set will be imported again and this time I will be removing the instant, dteday,cnt,yr,season, and weekday features. This is because each one of them have a direct relation with the date and would be meaningless. Heavy rain will be merged into rain due to it making up such a small part of the data. I will also have to normalize the **casual** and **registered** users  features for the clustering to properly be done. I will simply divide each value by the maximum value of the feature.

```{r}
pj<-read.csv("day.csv")
pj<-subset(pj, select = -c(instant, dteday, cnt,yr, season,weekday))
pj$weathersit<-ifelse(pj$weathersit == 4,3,pj$weathersit)
pj$casual<-pj$casual/max(pj$casual)
pj$registered<-pj$registered/max(pj$registered)
head(pj)
```
### Data Percentages

The following code will be find the percentage of holidays, workingdays, sunnydays, cloudydays, rainydays for each month

```{r}
#Percentage of days that are holidays for each month
holiday<-c(
sum(pj$holiday == 1 & pj$mnth == 1)/sum(pj$mnth ==1),
sum(pj$holiday == 1 & pj$mnth == 2)/sum(pj$mnth ==2),
sum(pj$holiday == 1 & pj$mnth == 3)/sum(pj$mnth ==3),
sum(pj$holiday == 1 & pj$mnth == 4)/sum(pj$mnth ==4),
sum(pj$holiday == 1 & pj$mnth == 5)/sum(pj$mnth ==5),
sum(pj$holiday == 1 & pj$mnth == 6)/sum(pj$mnth ==6),
sum(pj$holiday == 1 & pj$mnth == 7)/sum(pj$mnth ==7),
sum(pj$holiday == 1 & pj$mnth == 8)/sum(pj$mnth ==8),
sum(pj$holiday == 1 & pj$mnth == 9)/sum(pj$mnth ==9),
sum(pj$holiday == 1 & pj$mnth == 10)/sum(pj$mnth ==10),
sum(pj$holiday == 1 & pj$mnth == 11)/sum(pj$mnth ==11),
sum(pj$holiday == 1 & pj$mnth == 12)/sum(pj$mnth ==12))
#Percentage of days that are work days for each month
workingday<-c(
sum(pj$workingday == 1 & pj$mnth == 1)/sum(pj$mnth ==1),
sum(pj$workingday == 1 & pj$mnth == 2)/sum(pj$mnth ==2),
sum(pj$workingday == 1 & pj$mnth == 3)/sum(pj$mnth ==3),
sum(pj$workingday == 1 & pj$mnth == 4)/sum(pj$mnth ==4),
sum(pj$workingday == 1 & pj$mnth == 5)/sum(pj$mnth ==5),
sum(pj$workingday == 1 & pj$mnth == 6)/sum(pj$mnth ==6),
sum(pj$workingday == 1 & pj$mnth == 7)/sum(pj$mnth ==7),
sum(pj$workingday == 1 & pj$mnth == 8)/sum(pj$mnth ==8),
sum(pj$workingday == 1 & pj$mnth == 9)/sum(pj$mnth ==9),
sum(pj$workingday == 1 & pj$mnth == 10)/sum(pj$mnth ==10),
sum(pj$workingday == 1 & pj$mnth == 11)/sum(pj$mnth ==11),
sum(pj$workingday == 1 & pj$mnth == 12)/sum(pj$mnth ==12))
#percentage of days sunny each month
sunny<-c(
sum(pj$weathersit == 1 & pj$mnth == 1)/sum(pj$mnth ==1),
sum(pj$weathersit == 1 & pj$mnth == 2)/sum(pj$mnth ==2),
sum(pj$weathersit == 1 & pj$mnth == 3)/sum(pj$mnth ==3),
sum(pj$weathersit == 1 & pj$mnth == 4)/sum(pj$mnth ==4),
sum(pj$weathersit == 1 & pj$mnth == 5)/sum(pj$mnth ==5),
sum(pj$weathersit == 1 & pj$mnth == 6)/sum(pj$mnth ==6),
sum(pj$weathersit == 1 & pj$mnth == 7)/sum(pj$mnth ==7),
sum(pj$weathersit == 1 & pj$mnth == 8)/sum(pj$mnth ==8),
sum(pj$weathersit == 1 & pj$mnth == 9)/sum(pj$mnth ==9),
sum(pj$weathersit == 1 & pj$mnth == 10)/sum(pj$mnth ==10),
sum(pj$weathersit == 1 & pj$mnth == 11)/sum(pj$mnth ==11),
sum(pj$weathersit == 1 & pj$mnth == 12)/sum(pj$mnth ==12))
#percentage of days that are cloudy
cloudy<-c(
sum(pj$weathersit == 2 & pj$mnth == 1)/sum(pj$mnth ==1),
sum(pj$weathersit == 2 & pj$mnth == 2)/sum(pj$mnth ==2),
sum(pj$weathersit == 2 & pj$mnth == 3)/sum(pj$mnth ==3),
sum(pj$weathersit == 2 & pj$mnth == 4)/sum(pj$mnth ==4),
sum(pj$weathersit == 2 & pj$mnth == 5)/sum(pj$mnth ==5),
sum(pj$weathersit == 2 & pj$mnth == 6)/sum(pj$mnth ==6),
sum(pj$weathersit == 2 & pj$mnth == 7)/sum(pj$mnth ==7),
sum(pj$weathersit == 2 & pj$mnth == 8)/sum(pj$mnth ==8),
sum(pj$weathersit == 2 & pj$mnth == 9)/sum(pj$mnth ==9),
sum(pj$weathersit == 2 & pj$mnth == 10)/sum(pj$mnth ==10),
sum(pj$weathersit == 2 & pj$mnth == 11)/sum(pj$mnth ==11),
sum(pj$weathersit == 2 & pj$mnth == 12)/sum(pj$mnth ==12) 
)
#percentage of days that are rainy
rainy<-c(
sum(pj$weathersit == 3 & pj$mnth == 1)/sum(pj$mnth ==1),
sum(pj$weathersit == 3 & pj$mnth == 2)/sum(pj$mnth ==2),
sum(pj$weathersit == 3 & pj$mnth == 3)/sum(pj$mnth ==3),
sum(pj$weathersit == 3 & pj$mnth == 4)/sum(pj$mnth ==4),
sum(pj$weathersit == 3 & pj$mnth == 5)/sum(pj$mnth ==5),
sum(pj$weathersit == 3 & pj$mnth == 6)/sum(pj$mnth ==6),
sum(pj$weathersit == 3 & pj$mnth == 7)/sum(pj$mnth ==7),
sum(pj$weathersit == 3 & pj$mnth == 8)/sum(pj$mnth ==8),
sum(pj$weathersit == 3 & pj$mnth == 9)/sum(pj$mnth ==9),
sum(pj$weathersit == 3 & pj$mnth == 10)/sum(pj$mnth ==10),
sum(pj$weathersit == 3 & pj$mnth == 11)/sum(pj$mnth ==11),
sum(pj$weathersit == 3 & pj$mnth == 12)/sum(pj$mnth ==12))
```

The following code will find the average temp, atemp, hum, wind speed, casual, and registered values for each month.
```{r}
#Average temp for each month
temp<-c(
mean(pj[pj$mnth == 1, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 2, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 3, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 4, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 5, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 6, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 7, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 8, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 9, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 10, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 11, 'temp'], na.rm = TRUE),
mean(pj[pj$mnth == 12, 'temp'], na.rm = TRUE))
#Average atemp for each month
atemp<-c(
mean(pj[pj$mnth == 1, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 2, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 3, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 4, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 5, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 6, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 7, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 8, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 9, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 10, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 11, 'atemp'], na.rm = TRUE),
mean(pj[pj$mnth == 12, 'atemp'], na.rm = TRUE))
#Average humidity for each month
hum<-c(
mean(pj[pj$mnth == 1, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 2, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 3, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 4, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 5, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 6, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 7, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 8, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 9, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 10, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 11, 'hum'], na.rm = TRUE),
mean(pj[pj$mnth == 12, 'hum'], na.rm = TRUE))
#Average wind speed for each month
windspeed<-c(
mean(pj[pj$mnth == 1, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 2, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 3, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 4, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 5, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 6, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 7, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 8, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 9, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 10, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 11, 'windspeed'], na.rm = TRUE),
mean(pj[pj$mnth == 12, 'windspeed'], na.rm = TRUE))
#Average casual rider per month
casual<-c(
mean(pj[pj$mnth == 1, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 2, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 3, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 4, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 5, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 6, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 7, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 8, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 9, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 10, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 11, 'casual'], na.rm = TRUE),
mean(pj[pj$mnth == 12, 'casual'], na.rm = TRUE))
#Average Registered rider per month
registered<-c(
mean(pj[pj$mnth == 1, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 2, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 3, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 4, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 5, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 6, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 7, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 8, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 9, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 10, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 11, 'registered'], na.rm = TRUE),
mean(pj[pj$mnth == 12, 'registered'], na.rm = TRUE))
```

### Creating Dataframe

Now that all the needed information has been extracted from the raw dataset, I can merge it all together into on new dataset called **df**. The column names will be the names of the values I extracted and the row names will be the months of the year. 
```{r}
df<-as.data.frame(cbind(workingday,holiday,sunny,cloudy,rainy,temp,atemp,hum,windspeed,casual,registered))
rownames(df)<-c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')
df
```
Now, we have a 12X11 table with each value in a range of 0-1. This will allow for the euclidean distance formulas to be done properly.

## Building the model

### Euclidean Distance Matrix

Below is a matrix displaying the euclidean distances between each month. These numbers will be used when developing the Hierarchical clusters.

```{r}
(dist<-dist(df[1:11], method = "euclidean"))
```

### Developing Hierarachical Models

We will compare the five different options to see if it divides our data into different clusters while using k=4.

```{r}
hc1 <- hclust(dist, method = "single")
(memb1<-cutree(hc1, k=4))
hc2 <- hclust(dist, method = "average")
(memb2<-cutree(hc2, k=4))
hc3 <- hclust(dist, method = "complete")
(memb3<-cutree(hc3, k=4))
hc4 <- hclust(dist, method = "median")
(memb4<-cutree(hc4, k=4))
hc5 <- hclust(dist, method = "centroid")
(memb5<-cutree(hc5, k=4))
```
It appears that each method produced the same clusters except for the single linkage method.

## Evaluation

### Hierarachial plot

Below are the plots for each of the methods

```{r}
#Single Linkage
plot(hc1, hang = -1, ann = FALSE,)
abline(h=.21,col='red')
```
```{r}
#Average Linkage
plot(hc2, hang = -1, ann = FALSE)
abline(h=.245,col='red')
```
```{r}
#Complete Linkage
plot(hc3, hang = -1, ann = FALSE)
abline(h=.255,col='red')
```
```{r}
#Median Linkage
plot(hc4, hang = -1, ann = FALSE)
abline(h=.21,col='red')
```
```{r}
#Centroid Linkage
plot(hc5, hang = -1, ann = FALSE)
abline(h=.21,col='red')
```


### Heatmap

This is the heatmap for single linkage.

```{r}
df1<-df
row.names(df1) <- paste(memb1, ": ", row.names(df1), sep = "")
heatmap(as.matrix(df1), Colv = NA, hclustfun = hclust, col=rev(paste("grey",1:99,sep="")))
```

This is the heatmap for complete linkage
```{r}
df2<-df
row.names(df2) <- paste(memb3, ": ", row.names(df2), sep = "")
heatmap(as.matrix(df2), Colv = NA, hclustfun = hclust, col=rev(paste("gray",1:99,sep="")))
```

## Conclusion

We can tell that cluster 1 contains on average the lowest level of riders so doing a promotion during this time could bring in more business during that time.


```{r}
end<-Sys.time()
finish<- round((end - start), 3)
finish
```